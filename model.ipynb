{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df295ae6",
   "metadata": {},
   "source": [
    "# Proyek Analisis Sentimen Aplikasi BRIMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69275666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72000, 11)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "app_reviews_df = pd.read_csv('hasil_scraping.csv')\n",
    " \n",
    "app_reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ec9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5231b04b-3dbc-4b7f-862b-065af99bbae4</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>aplikasi buruk, merugikan, abal abal, duit say...</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>2.92.1</td>\n",
       "      <td>2026-01-21 11:12:22</td>\n",
       "      <td>Hai Sobat BRI, mohon maaf atas kendala top up ...</td>\n",
       "      <td>2026-01-21 11:06:59</td>\n",
       "      <td>2.92.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8fc2ddb6-2946-40f8-a0e7-9d4ad9ac0c93</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Bank besar,tp aplikasinya payah.KTP baret diki...</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2.92.1</td>\n",
       "      <td>2026-01-20 15:43:11</td>\n",
       "      <td>Hai Sobat BRI, mohon maaf atas kendala yang di...</td>\n",
       "      <td>2026-01-20 16:03:11</td>\n",
       "      <td>2.92.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c6fd8082-7ead-4e3f-b14d-4685733ee76f</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>setiap login disuruh masukan KTP lagi, jadi se...</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>2.92.0</td>\n",
       "      <td>2026-01-17 05:32:37</td>\n",
       "      <td>Hai Sobat BRI, mohon maaf atas kendala yang di...</td>\n",
       "      <td>2026-01-17 06:36:49</td>\n",
       "      <td>2.92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970f46c-5013-425f-949d-37565fb7343e</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Kenapa akhir2 ini brimo di hp saya, selalu set...</td>\n",
       "      <td>4</td>\n",
       "      <td>270</td>\n",
       "      <td>2.92.0</td>\n",
       "      <td>2026-01-09 12:59:50</td>\n",
       "      <td>Hai Sobat BRI, mohon maaf atas ketidaknyamanan...</td>\n",
       "      <td>2026-01-09 13:19:42</td>\n",
       "      <td>2.92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42de095a-495f-4bd7-bfed-8ed54403c5c0</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Saya kurangi 1 bintang sebab untuk login finge...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>2.92.1</td>\n",
       "      <td>2026-01-19 19:43:15</td>\n",
       "      <td>Hai Sobat BRI, terima kasih atas ulasannya. Se...</td>\n",
       "      <td>2024-05-04 16:35:01</td>\n",
       "      <td>2.92.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId         userName  \\\n",
       "0  5231b04b-3dbc-4b7f-862b-065af99bbae4  Pengguna Google   \n",
       "1  8fc2ddb6-2946-40f8-a0e7-9d4ad9ac0c93  Pengguna Google   \n",
       "2  c6fd8082-7ead-4e3f-b14d-4685733ee76f  Pengguna Google   \n",
       "3  1970f46c-5013-425f-949d-37565fb7343e  Pengguna Google   \n",
       "4  42de095a-495f-4bd7-bfed-8ed54403c5c0  Pengguna Google   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "1  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "2  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "3  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0  aplikasi buruk, merugikan, abal abal, duit say...      1             67   \n",
       "1  Bank besar,tp aplikasinya payah.KTP baret diki...      1             51   \n",
       "2  setiap login disuruh masukan KTP lagi, jadi se...      4             63   \n",
       "3  Kenapa akhir2 ini brimo di hp saya, selalu set...      4            270   \n",
       "4  Saya kurangi 1 bintang sebab untuk login finge...      4             14   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0               2.92.1  2026-01-21 11:12:22   \n",
       "1               2.92.1  2026-01-20 15:43:11   \n",
       "2               2.92.0  2026-01-17 05:32:37   \n",
       "3               2.92.0  2026-01-09 12:59:50   \n",
       "4               2.92.1  2026-01-19 19:43:15   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0  Hai Sobat BRI, mohon maaf atas kendala top up ...  2026-01-21 11:06:59   \n",
       "1  Hai Sobat BRI, mohon maaf atas kendala yang di...  2026-01-20 16:03:11   \n",
       "2  Hai Sobat BRI, mohon maaf atas kendala yang di...  2026-01-17 06:36:49   \n",
       "3  Hai Sobat BRI, mohon maaf atas ketidaknyamanan...  2026-01-09 13:19:42   \n",
       "4  Hai Sobat BRI, terima kasih atas ulasannya. Se...  2024-05-04 16:35:01   \n",
       "\n",
       "  appVersion  \n",
       "0     2.92.1  \n",
       "1     2.92.1  \n",
       "2     2.92.0  \n",
       "3     2.92.0  \n",
       "4     2.92.1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25953f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72000 entries, 0 to 71999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   reviewId              72000 non-null  object\n",
      " 1   userName              72000 non-null  object\n",
      " 2   userImage             72000 non-null  object\n",
      " 3   content               72000 non-null  object\n",
      " 4   score                 72000 non-null  int64 \n",
      " 5   thumbsUpCount         72000 non-null  int64 \n",
      " 6   reviewCreatedVersion  60689 non-null  object\n",
      " 7   at                    72000 non-null  object\n",
      " 8   replyContent          71875 non-null  object\n",
      " 9   repliedAt             71875 non-null  object\n",
      " 10  appVersion            60689 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "app_reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b32bd",
   "metadata": {},
   "source": [
    "### cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c76e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72000, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pilih kolom yang relevan untuk analisis sentimen\n",
    "columns_to_keep = ['reviewId', 'userName', 'content', 'score', 'thumbsUpCount', \n",
    "                   'reviewCreatedVersion', 'at', 'app_name', 'app_id']\n",
    "\n",
    "# Filter kolom yang ada\n",
    "available_columns = [col for col in columns_to_keep if col in app_reviews_df.columns]\n",
    "df = app_reviews_df[available_columns].copy()\n",
    "\n",
    "# Rename kolom agar lebih mudah dibaca\n",
    "df = df.rename(columns={\n",
    "    'reviewId': 'review_id',\n",
    "    'userName': 'user_name', \n",
    "    'content': 'review_text',\n",
    "    'score': 'rating',\n",
    "    'thumbsUpCount': 'thumbs_up',\n",
    "    'reviewCreatedVersion': 'app_version',\n",
    "    'at': 'review_date'\n",
    "})\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8b4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id          0\n",
       "user_name          0\n",
       "review_text        0\n",
       "rating             0\n",
       "thumbs_up          0\n",
       "app_version    11311\n",
       "review_date        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['review_text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81ab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(261)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = df.duplicated(subset=['review_text']).sum()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus duplikat berdasarkan kolom 'review_text'\n",
    "df = df.drop_duplicates(subset=['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d428250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b27f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL DATA SETELAH CLEANING: 71,739 reviews\n",
      "Memenuhi syarat minimal 10.000 sampel!\n"
     ]
    }
   ],
   "source": [
    "print(f\"TOTAL DATA SETELAH CLEANING: {len(df):,} reviews\")\n",
    "\n",
    "# Cek apakah memenuhi syarat minimal 10000\n",
    "if len(df) >= 10000:\n",
    "    print(\"Memenuhi syarat minimal 10.000 sampel!\")\n",
    "else:\n",
    "    print(\"Data kurang dari 10.000. Perlu scraping tambahan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eaba78",
   "metadata": {},
   "source": [
    "### pre processing text\n",
    "#### pelabelan data dengan lexicon dan ekstraksi fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_LEXICON = {\n",
    "    'bagus', 'baik', 'hebat', 'luar biasa', 'sempurna', 'mantap', 'mantab', 'mantul',\n",
    "    'keren', 'canggih', 'istimewa', 'unggul', 'prima', 'optimal', 'maksimal',\n",
    "    'fantastis', 'menakjubkan', 'mengagumkan', 'memukau', 'spektakuler',\n",
    "    'brilian', 'cemerlang', 'gemilang', 'mumpuni', 'andal', 'handal',\n",
    "\n",
    "    'puas', 'senang', 'suka', 'cinta', 'sayang', 'kagum', 'takjub', 'terpesona',\n",
    "    'bahagia', 'gembira', 'riang', 'sukacita', 'antusias', 'semangat',\n",
    "    'nyaman', 'tenang', 'aman', 'tenteram', 'damai', 'lega', 'plong',\n",
    "\n",
    "    'cepat', 'cepet', 'kilat', 'gesit', 'sigap', 'tanggap', 'responsif',\n",
    "    'mudah', 'gampang', 'simpel', 'sederhana', 'praktis', 'efisien', 'efektif',\n",
    "    'lancar', 'mulus', 'smooth', 'stabil', 'konsisten', 'reliable',\n",
    "    'lengkap', 'komplit', 'komprehensif', 'detail', 'rinci',\n",
    "    'murah', 'terjangkau', 'ekonomis', 'hemat', 'worth', 'worthit',\n",
    "    'gratis', 'free', 'bonus', 'promo', 'diskon', 'cashback',\n",
    "\n",
    "    'terima kasih', 'terimakasih', 'makasih', 'thanks', 'thx', 'tq',\n",
    "    'apresiasi', 'hargai', 'salut', 'acungi jempol', 'top', 'jos',\n",
    "    'recommended', 'recommend', 'rekomendasi', 'rekomen',\n",
    "    'terbaik', 'best', 'nomor satu', 'juara', 'champion',\n",
    "\n",
    "    'membantu', 'menolong', 'mendukung', 'memudahkan', 'mempermudah',\n",
    "    'memuaskan', 'menyenangkan', 'menghibur', 'menarik',\n",
    "    'berfungsi', 'bekerja', 'jalan', 'berjalan', 'aktif', 'hidup',\n",
    "    'berhasil', 'sukses', 'tercapai', 'terwujud', 'terlaksana',\n",
    "    'meningkat', 'berkembang', 'maju', 'progress', 'improve',\n",
    "\n",
    "    'mantep', 'mantabs', 'josss', 'joss', 'topcer', 'topp', 'oks', 'okee',\n",
    "    'asik', 'asyik', 'sip', 'siip', 'oke banget', 'kece', 'keceh',\n",
    "    'niceee', 'nice', 'good', 'great', 'awesome', 'amazing', 'excellent',\n",
    "    'perfect', 'wonderful', 'beautiful', 'love', 'lovely',\n",
    "\n",
    "    'update bagus', 'fitur lengkap', 'tampilan menarik', 'ui bagus', 'ux bagus',\n",
    "    'loading cepat', 'ringan', 'tidak lemot', 'tidak lag', 'anti lag',\n",
    "    'driver ramah', 'pelayanan bagus', 'respon cepat', 'fast response',\n",
    "    'pengiriman cepat', 'tepat waktu', 'on time', 'sesuai', 'akurat',\n",
    "    'aman', 'terpercaya', 'trusted', 'terjamin', 'garansi',\n",
    "}\n",
    "\n",
    "NEGATIVE_LEXICON = {\n",
    "    'jelek', 'buruk', 'busuk', 'bobrok', 'rusak', 'hancur', 'ancur',\n",
    "    'parah', 'payah', 'sampah', 'garbage', 'trash', 'zonk', 'zonkk',\n",
    "    'mengecewakan', 'kecewa', 'menyesal', 'nyesel', 'rugi', 'sia-sia',\n",
    "\n",
    "    'tidak puas', 'tidak suka', 'benci', 'muak', 'jijik', 'kesal',\n",
    "    'marah', 'emosi', 'geram', 'murka', 'berang', 'jengkel',\n",
    "    'frustrasi', 'frustasi', 'stress', 'stres', 'pusing', 'bingung',\n",
    "    'sedih', 'kecewa berat', 'menyedihkan', 'miris', 'tragis',\n",
    "\n",
    "    'error', 'eror', 'bug', 'bugs', 'crash', 'hang', 'freeze',\n",
    "    'lemot', 'lambat', 'lelet', 'lama', 'loading lama', 'lag', 'ngelag',\n",
    "    'force close', 'fc', 'keluar sendiri', 'menutup sendiri', 'mati',\n",
    "    'tidak bisa', 'gabisa', 'gagal', 'failed', 'fail',\n",
    "    'tidak jalan', 'tidak berfungsi', 'tidak bekerja', 'macet', 'stuck',\n",
    "    'blank', 'kosong', 'hilang', 'lost', 'missing',\n",
    "\n",
    "    'sulit', 'susah', 'ribet', 'rumit', 'kompleks', 'complicated',\n",
    "    'membingungkan', 'confusing', 'tidak jelas', 'ambigu',\n",
    "    'mahal', 'kemahalan', 'overpriced', 'tidak worth', 'boros',\n",
    "    'tidak lengkap', 'minim', 'terbatas', 'limited',\n",
    "\n",
    "    'tidak responsif', 'slow respon', 'slow response', 'lama respon',\n",
    "    'tidak membantu', 'tidak berguna', 'useless', 'percuma',\n",
    "\n",
    "    'bohong', 'tipu', 'nipu', 'penipuan', 'scam', 'fraud',\n",
    "    'curang', 'tidak jujur', 'tidak adil', 'merugikan',\n",
    "\n",
    "    'menghapus', 'hapus', 'uninstall', 'delete', 'buang',\n",
    "    'menjengkelkan', 'menyebalkan', 'mengganggu',\n",
    "    'merusak', 'menghancurkan', 'memperburuk', 'memperparah',\n",
    "\n",
    "    'bangsat', 'kampret', 'sialan', 'brengsek', 'tolol', 'bodoh', 'goblok',\n",
    "    'gak guna', 'gak berguna', 'sampah banget', 'parah banget',\n",
    "    'kapok', 'ogah', 'males', 'malas', 'enggan',\n",
    "    'worst', 'bad', 'terrible', 'horrible', 'awful', 'sucks', 'suck',\n",
    "\n",
    "    'update jelek', 'update parah', 'makin jelek', 'makin buruk',\n",
    "    'driver kasar', 'driver tidak ramah', 'pelayanan buruk',\n",
    "    'pengiriman lama', 'terlambat', 'telat', 'tidak sesuai',\n",
    "    'barang rusak', 'barang salah', 'tidak sampai', 'refund lama',\n",
    "    'akun diblokir', 'banned', 'suspend', 'tidak bisa login',\n",
    "}\n",
    "\n",
    "NEGATION_WORDS = {\n",
    "    'tidak', 'tak', 'bukan', 'belum', 'jangan', 'tanpa', 'tiada',\n",
    "    'gak', 'ga', 'gk', 'ngga', 'nggak', 'kagak', 'kaga', 'enggak',\n",
    "    'tdk', 'blm', 'jgn', 'gx', 'g',\n",
    "    'never', 'no', 'not', 'none', 'nothing', 'neither', 'nobody'\n",
    "}\n",
    "\n",
    "SOFT_NEGATORS = {'kurang'}\n",
    "\n",
    "INTENSIFIER_WORDS = {\n",
    "    'sangat': 1.5, 'amat': 1.5, 'sekali': 1.5, 'banget': 1.5, 'bgt': 1.5,\n",
    "    'super': 1.8, 'ultra': 1.8, 'extra': 1.5, 'ekstra': 1.5,\n",
    "    'terlalu': 1.3, 'paling': 1.5, 'ter': 1.3,\n",
    "    'luar biasa': 2.0, 'extremely': 2.0, 'very': 1.5, 'really': 1.5,\n",
    "    'totally': 1.8, 'absolutely': 2.0, 'completely': 1.8,\n",
    "    'sedikit': 0.5, 'agak': 0.7, 'cukup': 0.8, 'lumayan': 0.7,\n",
    "    'hampir': 0.6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_ID = {\n",
    "    'saya', 'aku', 'kamu', 'anda', 'dia', 'ia', 'kami', 'kita', 'mereka',\n",
    "    'ini', 'itu', 'sini', 'situ', 'sana',\n",
    "    'dan', 'atau', 'tetapi', 'tapi', 'namun', 'melainkan', 'padahal',\n",
    "    'sedangkan', 'sementara', 'sebaliknya', 'bahkan', 'lagipula',    \n",
    "    'di', 'ke', 'dari', 'pada', 'dalam', 'dengan', 'untuk', 'oleh',\n",
    "    'tentang', 'terhadap', 'mengenai', 'sebagai', 'seperti', 'bagai',\n",
    "    'sudah', 'telah', 'belum', 'akan', 'sedang', 'masih', 'pernah',\n",
    "    'selalu', 'sering', 'jarang', 'kadang', 'hanya', 'cuma', 'saja',\n",
    "    'juga', 'pun', 'lagi', 'lalu', 'kemudian', 'setelah', 'sebelum',\n",
    "    'sambil', 'seraya', 'ketika', 'saat', 'waktu', 'bila', 'jika',\n",
    "    'kalau', 'apabila', 'karena', 'sebab', 'akibat', 'sehingga',\n",
    "    'supaya', 'agar', 'biar', 'meski', 'meskipun', 'walaupun', 'biarpun',\n",
    "    'apa', 'siapa', 'mana', 'kapan', 'kenapa', 'mengapa', 'bagaimana', 'berapa',\n",
    "    'ada', 'adalah', 'ialah', 'yaitu', 'yakni', 'merupakan',\n",
    "    'bisa', 'dapat', 'mampu', 'sanggup', 'boleh', 'harus', 'wajib', 'perlu', 'mesti',\n",
    "    'ingin', 'mau', 'hendak',\n",
    "    'satu', 'dua', 'tiga', 'empat', 'lima', 'enam', 'tujuh', 'delapan', 'sembilan', 'sepuluh',\n",
    "    'pertama', 'kedua', 'ketiga', 'banyak', 'sedikit', 'beberapa', 'semua', 'seluruh', 'setiap',\n",
    "    'yg', 'nya', 'gak', 'ga', 'gk', 'ngga', 'nggak', 'kagak', 'enggak',\n",
    "    'udah', 'udh', 'dah', 'sdh', 'uda',\n",
    "    'aja', 'aj', 'doang', 'dong', 'deh', 'sih', 'nih', 'tuh', 'kok', 'kan', 'loh', 'lah',\n",
    "    'banget', 'bgt', 'bngt', 'sangat', 'sekali', 'amat',\n",
    "    'gitu', 'gini', 'begitu', 'begini',\n",
    "    'gimana', 'gmn', 'gmna', 'bagaimana',\n",
    "    'kayak', 'kyk', 'seperti',\n",
    "    'emang', 'emg', 'memang',\n",
    "    'aja', 'saja', 'cuma', 'cuman', 'hanya',\n",
    "    'lg', 'lagi', 'lgi',\n",
    "    'sm', 'sama', 'dgn', 'dengan',\n",
    "    'tp', 'tpi', 'tapi', 'tetapi',\n",
    "    'kalo', 'kl', 'klu', 'kalau',\n",
    "    'krn', 'karena', 'soalnya',\n",
    "    'utk', 'untuk', 'buat', 'uat',\n",
    "    'jd', 'jadi', 'jadinya',\n",
    "    'jg', 'jga', 'juga',\n",
    "    'bs', 'bsa', 'bisa',\n",
    "    'org', 'orang',\n",
    "    'sy', 'sya', 'saya',\n",
    "    'gw', 'gue', 'gua', 'w',\n",
    "    'lu', 'lo', 'elu', 'elo',\n",
    "    'app', 'apps', 'aplikasi', 'apk', 'application',\n",
    "    'download', 'instal', 'install', 'update', 'version', 'versi',\n",
    "    'bintang', 'star', 'rating', 'review', 'ulasan',\n",
    "    'ya', 'yaa', 'yaaa', 'iya', 'iyaa', 'ok', 'oke', 'okay', 'okey',\n",
    "    'oh', 'ohh', 'ah', 'ahh', 'eh', 'ehh', 'uh', 'uhh',\n",
    "    'wah', 'wahh', 'wow', 'woww', 'yay', 'hore',\n",
    "    'hmm', 'hmmm', 'mmm', 'emm',\n",
    "    'haha', 'hahaha', 'hihi', 'hehe', 'wkwk', 'wkwkwk', 'kwkw', 'awkwk',\n",
    "    'the', 'and', 'is', 'it', 'to', 'of', 'in', 'for', 'on', 'this', 'that',\n",
    "    'be', 'are', 'was', 'were', 'been', 'have', 'has', 'had',\n",
    "    'but', 'not', 'you', 'your', 'we', 'our', 'they', 'their',\n",
    "    'so', 'if', 'or', 'as', 'at', 'by', 'an', 'no', 'yes',\n",
    "    'very', 'just', 'only', 'also', 'even', 'still', 'too', 'more',\n",
    "    'good', 'bad', 'nice', 'great', 'best', 'worst', 'please', 'thanks', 'thank'\n",
    "}\n",
    "\n",
    "SLANG_DICT = {\n",
    "    'gk': 'tidak', 'gak': 'tidak', 'ga': 'tidak', 'tdk': 'tidak', 'gx': 'tidak',\n",
    "    'ngga': 'tidak', 'nggak': 'tidak', 'kagak': 'tidak', 'kaga': 'tidak',\n",
    "    'g': 'tidak', 'enggak': 'tidak', 'engga': 'tidak', 'tak': 'tidak',\n",
    "    'udh': 'sudah', 'udah': 'sudah', 'sdh': 'sudah', 'uda': 'sudah', 'dah': 'sudah',\n",
    "    'mantap': 'bagus', 'mantab': 'bagus', 'mantul': 'bagus', 'mantaps': 'bagus',\n",
    "    'keren': 'bagus', 'cakep': 'bagus', 'oks': 'bagus', 'okee': 'bagus',\n",
    "    'jos': 'bagus', 'josss': 'bagus', 'top': 'bagus', 'toppp': 'bagus',\n",
    "    'recommended': 'rekomendasi', 'recommend': 'rekomendasi', 'recomended': 'rekomendasi',\n",
    "    'worthed': 'worth', 'worthit': 'worth',\n",
    "    'helpful': 'membantu', 'useful': 'berguna',\n",
    "    'lancar': 'lancar', 'lancarr': 'lancar', 'lncar': 'lancar',\n",
    "    'cepet': 'cepat', 'cpet': 'cepat', 'cpt': 'cepat',\n",
    "    'gampang': 'mudah', 'gmpang': 'mudah', 'simpel': 'mudah', 'simple': 'mudah',\n",
    "    'makasih': 'terima kasih', 'makasi': 'terima kasih', 'mksh': 'terima kasih',\n",
    "    'thx': 'terima kasih', 'thanks': 'terima kasih', 'tq': 'terima kasih',\n",
    "    'jelek': 'buruk', 'jlek': 'buruk', 'jelekk': 'buruk',\n",
    "    'parah': 'buruk', 'parahhh': 'buruk', 'prah': 'buruk',\n",
    "    'ancur': 'rusak', 'hancur': 'rusak', 'ancuurr': 'rusak',\n",
    "    'payah': 'buruk', 'payahh': 'buruk',\n",
    "    'zonk': 'buruk', 'zonkk': 'buruk',\n",
    "    'sampah': 'buruk', 'smph': 'buruk',\n",
    "    'lemot': 'lambat', 'lelet': 'lambat', 'lmot': 'lambat', 'lemott': 'lambat',\n",
    "    'lama': 'lambat', 'lamaa': 'lambat', 'lma': 'lambat',\n",
    "    'ribet': 'sulit', 'rbet': 'sulit', 'ribett': 'sulit',\n",
    "    'susah': 'sulit', 'ssah': 'sulit', 'susahh': 'sulit',\n",
    "    'error': 'error', 'eror': 'error', 'erorr': 'error',\n",
    "    'bug': 'error', 'bugs': 'error', 'bugg': 'error',\n",
    "    'crash': 'error', 'cras': 'error', 'crashh': 'error',\n",
    "    'hang': 'error', 'ngehang': 'error', 'hangg': 'error',\n",
    "    'force close': 'error', 'fc': 'error',\n",
    "    'kecewa': 'kecewa', 'kcwa': 'kecewa', 'kcewaaa': 'kecewa',\n",
    "    'nyesel': 'menyesal', 'nysel': 'menyesal', 'nyesell': 'menyesal',\n",
    "    'rugi': 'rugi', 'rugii': 'rugi',\n",
    "    'kapok': 'kapok', 'kapokk': 'kapok',\n",
    "    'bgt': 'sangat', 'bngt': 'sangat', 'bngtt': 'sangat', 'bgtt': 'sangat',\n",
    "    'bkn': 'bukan', 'blm': 'belum', 'blum': 'belum', 'blom': 'belum',\n",
    "    'bs': 'bisa', 'bsa': 'bisa',\n",
    "    'org': 'orang', 'orng': 'orang', 'orngg': 'orang',\n",
    "    'bgus': 'bagus', 'bgs': 'bagus', 'baguss': 'bagus',\n",
    "    'krg': 'kurang', 'kurang': 'kurang', 'krangg': 'kurang',\n",
    "    'pdhl': 'padahal', 'pdhal': 'padahal',\n",
    "    'smoga': 'semoga', 'moga': 'semoga', 'smga': 'semoga',\n",
    "    'knp': 'kenapa', 'knapa': 'kenapa',\n",
    "    'gmna': 'bagaimana', 'gmn': 'bagaimana',\n",
    "    'bner': 'benar', 'bnr': 'benar', 'bener': 'benar',\n",
    "    'slalu': 'selalu', 'sllu': 'selalu', 'sellalu': 'selalu',\n",
    "    'pke': 'pakai', 'pkai': 'pakai', 'pk': 'pakai', 'make': 'pakai',\n",
    "    'dri': 'dari', 'dr': 'dari',\n",
    "    'yg': 'yang', 'yng': 'yang',\n",
    "    'dgn': 'dengan', 'dngn': 'dengan',\n",
    "    'utk': 'untuk', 'untk': 'untuk', 'u': 'untuk',\n",
    "    'krn': 'karena', 'karna': 'karena', 'krna': 'karena',\n",
    "    'klo': 'kalau', 'klu': 'kalau', 'kl': 'kalau',\n",
    "    'jgn': 'jangan', 'jngn': 'jangan',\n",
    "    'hrs': 'harus', 'hrus': 'harus',\n",
    "    'trs': 'terus', 'trus': 'terus', 'teros': 'terus',\n",
    "    'abis': 'habis', 'abiss': 'habis', 'abs': 'habis',\n",
    "    'bru': 'baru', 'baru': 'baru',\n",
    "    'ampe': 'sampai', 'sampe': 'sampai', 'smpe': 'sampai',\n",
    "    'bgt': 'sangat', 'sgt': 'sangat',\n",
    "    'bnyk': 'banyak', 'byk': 'banyak', 'bnyak': 'banyak'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_re_repeat = re.compile(r'(.)\\1{2,}')\n",
    "\n",
    "def _normalize_token(tok: str) -> str:\n",
    "    tok = tok.lower().strip()\n",
    "    tok = _re_repeat.sub(r'\\1', tok)\n",
    "    return tok\n",
    "\n",
    "def _tokenize(text: str):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    raw = re.findall(r\"[\\w']+\", text.lower())\n",
    "    return [_normalize_token(t) for t in raw if t.strip()]\n",
    "\n",
    "\n",
    "def calculate_sentiment_score(\n",
    "    text,\n",
    "    positive_lex,\n",
    "    negative_lex,\n",
    "    negation_words,\n",
    "    intensifiers,\n",
    "    neg_window=3,\n",
    "    intens_window=2,\n",
    "    soft_negators=None,\n",
    "    soft_factor=0.6\n",
    "):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return {'positive': 0.0, 'negative': 0.0, 'score': 0.0, 'hits': 0}\n",
    "\n",
    "    soft_negators = set(soft_negators or [])\n",
    "    tokens = _tokenize(text)\n",
    "\n",
    "    pos = 0.0\n",
    "    neg = 0.0\n",
    "    hits = 0\n",
    "\n",
    "    neg_countdown = 0\n",
    "    neg_scale = 1.0\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "\n",
    "        # handle negator/soft-negator + cek bigram negator kata\n",
    "        if (tok in negation_words) or (tok in soft_negators):\n",
    "            if i < len(tokens) - 1:\n",
    "                bg = f\"{tok} {tokens[i+1]}\"\n",
    "                if bg in positive_lex:\n",
    "                    hits += 1\n",
    "                    pos += 1.0\n",
    "                    i += 2\n",
    "                    neg_countdown = 0\n",
    "                    neg_scale = 1.0\n",
    "                    continue\n",
    "                if bg in negative_lex:\n",
    "                    hits += 1\n",
    "                    neg += 1.0\n",
    "                    i += 2\n",
    "                    neg_countdown = 0\n",
    "                    neg_scale = 1.0\n",
    "                    continue\n",
    "\n",
    "            neg_countdown = neg_window\n",
    "            neg_scale = soft_factor if tok in soft_negators else 1.0\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # handle status negasi\n",
    "        is_negated = neg_countdown > 0\n",
    "        flip_scale = neg_scale if is_negated else 1.0\n",
    "\n",
    "        if neg_countdown > 0:\n",
    "            neg_countdown -= 1\n",
    "            if neg_countdown == 0:\n",
    "                neg_scale = 1.0\n",
    "\n",
    "        # handle intensifier multiplier (unigram + bigram intensifier)\n",
    "        intensity = 1.0\n",
    "        start_j = max(0, i - intens_window)\n",
    "        for j in range(start_j, i):\n",
    "            tprev = tokens[j]\n",
    "            if tprev in intensifiers:\n",
    "                intensity *= float(intensifiers[tprev])\n",
    "\n",
    "            if j - 1 >= 0:\n",
    "                prev_bg = f\"{tokens[j-1]} {tokens[j]}\"\n",
    "                if prev_bg in intensifiers:\n",
    "                    intensity *= float(intensifiers[prev_bg])\n",
    "\n",
    "        # handle bigram lexicon\n",
    "        used = False\n",
    "        if i < len(tokens) - 1:\n",
    "            bg = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "\n",
    "            if bg in positive_lex:\n",
    "                hits += 1\n",
    "                if is_negated:\n",
    "                    neg += 1.0 * intensity * flip_scale\n",
    "                else:\n",
    "                    pos += 1.0 * intensity\n",
    "                i += 2\n",
    "                used = True\n",
    "\n",
    "            elif bg in negative_lex:\n",
    "                hits += 1\n",
    "                if is_negated:\n",
    "                    pos += 0.8 * intensity * flip_scale\n",
    "                else:\n",
    "                    neg += 1.0 * intensity\n",
    "                i += 2\n",
    "                used = True\n",
    "\n",
    "        if used:\n",
    "            continue\n",
    "\n",
    "        # handle nigram\n",
    "        if tok in positive_lex:\n",
    "            hits += 1\n",
    "            if is_negated:\n",
    "                neg += 1.0 * intensity * flip_scale\n",
    "            else:\n",
    "                pos += 1.0 * intensity\n",
    "\n",
    "        elif tok in negative_lex:\n",
    "            hits += 1\n",
    "            if is_negated:\n",
    "                pos += 0.8 * intensity * flip_scale\n",
    "            else:\n",
    "                neg += 1.0 * intensity\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    score = pos - neg\n",
    "    return {'positive': pos, 'negative': neg, 'score': score, 'hits': hits}\n",
    "\n",
    "\n",
    "def assign_sentiment_label(score, hits, thr=0.25):\n",
    "    if hits == 0:\n",
    "        return 'Netral', 0.0\n",
    "\n",
    "    norm = score / max(1.0, float(hits))\n",
    "    if norm > thr:\n",
    "        return 'Positif', abs(norm)\n",
    "    elif norm < -thr:\n",
    "        return 'Negatif', abs(norm)\n",
    "    else:\n",
    "        return 'Netral', abs(norm)\n",
    "\n",
    "\n",
    "def label_sentiment_lexicon(text, strong_only=False, strong_thr=0.45, thr=0.25):\n",
    "    result = calculate_sentiment_score(\n",
    "        text,\n",
    "        POSITIVE_LEXICON,\n",
    "        NEGATIVE_LEXICON,\n",
    "        NEGATION_WORDS,\n",
    "        INTENSIFIER_WORDS,\n",
    "        neg_window=3,\n",
    "        intens_window=2,\n",
    "        soft_negators=SOFT_NEGATORS,\n",
    "        soft_factor=0.6\n",
    "    )\n",
    "\n",
    "    label, conf = assign_sentiment_label(result['score'], result['hits'], thr=thr)\n",
    "\n",
    "    if strong_only and conf < strong_thr:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'sentiment': label,\n",
    "        'confidence': conf,\n",
    "        'score': result['score'],\n",
    "        'hits': result['hits'],\n",
    "        'positive_score': result['positive'],\n",
    "        'negative_score': result['negative'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "URL_RE = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "HASHTAG_RE = re.compile(r'#(\\w+)')\n",
    "DIGIT_RE = re.compile(r'\\d+')\n",
    "REPEAT_RE = re.compile(r'(.)\\1{2,}')\n",
    "WS_RE = re.compile(r'\\s+')\n",
    "\n",
    "EMOJI_RE = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"\n",
    "    u\"\\U0001F300-\\U0001F5FF\"\n",
    "    u\"\\U0001F680-\\U0001F6FF\"\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "\"]+\", flags=re.UNICODE)\n",
    "\n",
    "# pisahkan yang frasa dan yang token\n",
    "PHRASE_SLANG = {k: v for k, v in SLANG_DICT.items() if ' ' in k}\n",
    "TOKEN_SLANG  = {k: v for k, v in SLANG_DICT.items() if ' ' not in k}\n",
    "\n",
    "PHRASE_PATTERNS = [\n",
    "    (re.compile(r'\\b' + re.escape(k) + r'\\b'), v)\n",
    "    for k, v in PHRASE_SLANG.items()\n",
    "]\n",
    "\n",
    "# whitelist token penting supaya tidak terhapus stopwords\n",
    "KEEP_TOKENS = set()\n",
    "ALL_TERMS = (\n",
    "    set(POSITIVE_LEXICON)\n",
    "    | set(NEGATIVE_LEXICON)\n",
    "    | set(NEGATION_WORDS)\n",
    "    | set(SOFT_NEGATORS)\n",
    "    | set(INTENSIFIER_WORDS.keys())\n",
    ")\n",
    "\n",
    "for term in ALL_TERMS:\n",
    "    for w in str(term).split():\n",
    "        KEEP_TOKENS.add(w)\n",
    "\n",
    "# punctuation jadi spasi\n",
    "PUNCT_TABLE = str.maketrans({c: \" \" for c in string.punctuation})\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # buang url/mention, hashtag tetap jadi kata\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = MENTION_RE.sub(\" \", text)\n",
    "    text = HASHTAG_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    # handle slang frasa\n",
    "    for pat, repl in PHRASE_PATTERNS:\n",
    "        text = pat.sub(repl, text)\n",
    "\n",
    "    # handle emoji dan angka\n",
    "    text = EMOJI_RE.sub(\" \", text)\n",
    "    text = DIGIT_RE.sub(\" \", text)\n",
    "\n",
    "    # handle punctuation, huruf berulang, spasi\n",
    "    text = text.translate(PUNCT_TABLE)\n",
    "    text = REPEAT_RE.sub(r\"\\1\", text)\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    # handle slang token (1 kata)\n",
    "    tokens = [TOKEN_SLANG.get(t, t) for t in tokens]\n",
    "\n",
    "    # handle kalau hasil mapping jadi multi-kata, pecah lagi\n",
    "    tokens = \" \".join(tokens).split()\n",
    "\n",
    "    # handle stopword removal + whitelist token penting\n",
    "    tokens = [t for t in tokens if (t in KEEP_TOKENS) or (t not in STOPWORDS_ID)]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 71739/71739 [00:01<00:00, 38283.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data setelah preprocess: 71717\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aplikasi buruk, merugikan, abal abal, duit say...</td>\n",
       "      <td>buruk merugikan abal abal duit hilang topup e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bank besar,tp aplikasinya payah.KTP baret diki...</td>\n",
       "      <td>bank besar aplikasinya buruk ktp baret dikit t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setiap login disuruh masukan KTP lagi, jadi se...</td>\n",
       "      <td>login disuruh masukan ktp hari bisa x transaks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kenapa akhir2 ini brimo di hp saya, selalu set...</td>\n",
       "      <td>akhir brimo hp login berubah putih layarnya ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saya kurangi 1 bintang sebab untuk login finge...</td>\n",
       "      <td>kurangi login finger handphone realme tidak mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  \\\n",
       "0  aplikasi buruk, merugikan, abal abal, duit say...   \n",
       "1  Bank besar,tp aplikasinya payah.KTP baret diki...   \n",
       "2  setiap login disuruh masukan KTP lagi, jadi se...   \n",
       "3  Kenapa akhir2 ini brimo di hp saya, selalu set...   \n",
       "4  Saya kurangi 1 bintang sebab untuk login finge...   \n",
       "\n",
       "                                        review_clean  \n",
       "0  buruk merugikan abal abal duit hilang topup e ...  \n",
       "1  bank besar aplikasinya buruk ktp baret dikit t...  \n",
       "2  login disuruh masukan ktp hari bisa x transaks...  \n",
       "3  akhir brimo hp login berubah putih layarnya ba...  \n",
       "4  kurangi login finger handphone realme tidak mu...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocessing\")\n",
    "\n",
    "df[\"review_clean\"] = df[\"review_text\"].progress_apply(preprocess_text)\n",
    "\n",
    "# buang yang kosong\n",
    "df = df[df[\"review_clean\"].fillna(\"\").str.strip().ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "print(\"Jumlah data setelah preprocess:\", len(df))\n",
    "df[[\"review_text\", \"review_clean\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c64a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Strong Pseudo-Labeling: 100%|██████████| 71717/71717 [00:02<00:00, 29059.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "Negatif    15957\n",
      "Positif    12853\n",
      "Name: count, dtype: int64\n",
      "Total kandidat training: 28810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Strong Pseudo-Labeling\")\n",
    "\n",
    "def get_strong_binary_label(text):\n",
    "    res = label_sentiment_lexicon(text, strong_only=True, strong_thr=0.60, thr=0.30)\n",
    "    if res is None:\n",
    "        return None\n",
    "    if res[\"sentiment\"] not in (\"Positif\", \"Negatif\"):\n",
    "        return None\n",
    "    if res[\"hits\"] < 2:\n",
    "        return None\n",
    "    return res\n",
    "\n",
    "df_lex = df.copy()\n",
    "df_lex[\"lex\"] = df_lex[\"review_clean\"].progress_apply(get_strong_binary_label)\n",
    "df_lex = df_lex[df_lex[\"lex\"].notna()].reset_index(drop=True)\n",
    "\n",
    "df_lex[\"sentiment\"]  = df_lex[\"lex\"].apply(lambda d: d[\"sentiment\"])\n",
    "df_lex[\"confidence\"] = df_lex[\"lex\"].apply(lambda d: d[\"confidence\"])\n",
    "df_lex[\"hits\"]       = df_lex[\"lex\"].apply(lambda d: d[\"hits\"])\n",
    "\n",
    "print(df_lex[\"sentiment\"].value_counts())\n",
    "print(\"Total kandidat training:\", len(df_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eba551",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f850b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "SKEMA_CONFIG = {\n",
    "    \"Skema 1\": {\n",
    "        \"model\": \"LinearSVC\",\n",
    "        \"model_class\": LinearSVC(C=1.2, class_weight=\"balanced\", max_iter=20000, dual=True),\n",
    "        \"word_ngram\": (1, 2),\n",
    "        \"word_max_features\": 40000,\n",
    "        \"word_min_df\": 2,\n",
    "        \"word_max_df\": 0.90,\n",
    "        \"char_ngram\": (3, 5),\n",
    "        \"char_max_features\": 60000,\n",
    "        \"char_min_df\": 2,\n",
    "        \"char_max_df\": 0.95,\n",
    "        \"split\": 0.20,\n",
    "    },\n",
    "    \"Skema 2\": { \n",
    "        \"model\": \"SGDClassifier(hinge)\",\n",
    "        \"model_class\": SGDClassifier(\n",
    "            loss=\"hinge\",\n",
    "            alpha=1e-5,\n",
    "            penalty=\"l2\",\n",
    "            max_iter=2000,\n",
    "            tol=1e-3,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=5,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42,\n",
    "        ),\n",
    "        \"word_ngram\": (1, 2),\n",
    "        \"word_max_features\": 50000,\n",
    "        \"word_min_df\": 2,\n",
    "        \"word_max_df\": 0.90,\n",
    "        \"char_ngram\": (3, 5),\n",
    "        \"char_max_features\": 60000,\n",
    "        \"char_min_df\": 2,\n",
    "        \"char_max_df\": 0.95,\n",
    "        \"split\": 0.20,\n",
    "    },\n",
    "    \"Skema 3\": {\n",
    "        \"model\": \"LogisticRegression(saga)\",\n",
    "        \"model_class\": LogisticRegression(\n",
    "            C=2.0, max_iter=8000, class_weight=\"balanced\",\n",
    "            solver=\"saga\", random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \"word_ngram\": (1, 2),\n",
    "        \"word_max_features\": 50000,\n",
    "        \"word_min_df\": 2,\n",
    "        \"word_max_df\": 0.90,\n",
    "        \"char_ngram\": (3, 5),\n",
    "        \"char_max_features\": 70000,\n",
    "        \"char_min_df\": 2,\n",
    "        \"char_max_df\": 0.95,\n",
    "        \"split\": 0.20,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ac518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skema    Model                   TrainAcc   TestAcc   F1macro     PrecM      RecM\n",
      "--------------------------------------------------------------------------------\n",
      "Skema 1  LinearSVC                 1.0000    0.9875    0.9873    0.9879    0.9869\n",
      "Skema 2  SGDClassifier(hinge)      0.9987    0.9859    0.9858    0.9861    0.9855\n",
      "Skema 3  LogisticRegression(saga)    0.9946    0.9861    0.9859    0.9867    0.9853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# pakai df_lex karena hasil strong pseudo-label\n",
    "X = df_lex[\"review_clean\"].astype(str).values\n",
    "y = df_lex[\"sentiment\"].astype(str).values\n",
    "w = df_lex[\"confidence\"].astype(float).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "def make_union(cfg):\n",
    "    word_vec = TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=cfg[\"word_ngram\"],\n",
    "        min_df=cfg[\"word_min_df\"],\n",
    "        max_df=cfg[\"word_max_df\"],\n",
    "        max_features=cfg[\"word_max_features\"],\n",
    "        sublinear_tf=True,\n",
    "        lowercase=False,\n",
    "    )\n",
    "    char_vec = TfidfVectorizer(\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=cfg[\"char_ngram\"],\n",
    "        min_df=cfg[\"char_min_df\"],\n",
    "        max_df=cfg[\"char_max_df\"],\n",
    "        max_features=cfg[\"char_max_features\"],\n",
    "        sublinear_tf=True,\n",
    "        lowercase=False,\n",
    "    )\n",
    "    return FeatureUnion([(\"word\", word_vec), (\"char\", char_vec)])\n",
    "\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X, y_enc, w, test_size=0.20, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(f\"{'Skema':<8} {'Model':<22} {'TrainAcc':>9} {'TestAcc':>9} {'F1macro':>9} {'PrecM':>9} {'RecM':>9}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, cfg in SKEMA_CONFIG.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"vec\", make_union(cfg)),\n",
    "        (\"clf\", cfg[\"model_class\"])\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train, clf__sample_weight=w_train)\n",
    "\n",
    "    pred_train = pipe.predict(X_train)\n",
    "    pred_test  = pipe.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, pred_train)\n",
    "    test_acc  = accuracy_score(y_test, pred_test)\n",
    "    f1m   = f1_score(y_test, pred_test, average=\"macro\")\n",
    "    precm = precision_score(y_test, pred_test, average=\"macro\", zero_division=0)\n",
    "    recm  = recall_score(y_test, pred_test, average=\"macro\", zero_division=0)\n",
    "\n",
    "    results[name] = {\n",
    "        \"pipe\": pipe,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"f1_macro\": f1m\n",
    "    }\n",
    "\n",
    "    print(f\"{name:<8} {cfg['model']:<22} {train_acc:9.4f} {test_acc:9.4f} {f1m:9.4f} {precm:9.4f} {recm:9.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4279484",
   "metadata": {},
   "source": [
    "### inferensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e544d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST SKEMA: Skema 1 | TestAcc: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# ambil skema terbaik dari test_acc dan f1_macro\n",
    "best_name = max(results, key=lambda k: (results[k][\"test_acc\"], results[k][\"f1_macro\"]))\n",
    "best_pipe = results[best_name][\"pipe\"]\n",
    "\n",
    "print(\"BEST SKEMA:\", best_name, \"| TestAcc:\", round(results[best_name][\"test_acc\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_clean</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gimn ktnya link d krm k kontak sms nga masuk h...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sangat membantu mo transaksi apapun lancar tan...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Positif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pendaftaran tolong bisa permudah coba daftar u...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Positif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selama mudah aman semoga aman</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Positif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>menyebalkan tadi siang coba video rekam detik ...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aman nyaman yang pasti cocok yang punya mobili...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Positif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bertransaksi brimo tidak bisa saldo apakah sal...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sejauh aplikasinya sangat bagus transaksi muda...</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Positif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brimo sekarang sulit banget tulisan keluar dul...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>update tampilan lebih fresh error tulisan gene...</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        review_clean   actual predicted  \\\n",
       "0  gimn ktnya link d krm k kontak sms nga masuk h...  Negatif   Negatif   \n",
       "1  sangat membantu mo transaksi apapun lancar tan...  Positif   Positif   \n",
       "2  pendaftaran tolong bisa permudah coba daftar u...  Positif   Positif   \n",
       "3                      selama mudah aman semoga aman  Positif   Positif   \n",
       "4  menyebalkan tadi siang coba video rekam detik ...  Negatif   Negatif   \n",
       "5  aman nyaman yang pasti cocok yang punya mobili...  Positif   Positif   \n",
       "6  bertransaksi brimo tidak bisa saldo apakah sal...  Negatif   Negatif   \n",
       "7  sejauh aplikasinya sangat bagus transaksi muda...  Positif   Positif   \n",
       "8  brimo sekarang sulit banget tulisan keluar dul...  Negatif   Negatif   \n",
       "9  update tampilan lebih fresh error tulisan gene...  Negatif   Negatif   \n",
       "\n",
       "   correct  \n",
       "0     True  \n",
       "1     True  \n",
       "2     True  \n",
       "3     True  \n",
       "4     True  \n",
       "5     True  \n",
       "6     True  \n",
       "7     True  \n",
       "8     True  \n",
       "9     True  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_sentiment(text, pipeline, label_encoder, do_preprocess=True):\n",
    "    cleaned = preprocess_text(text) if do_preprocess else text\n",
    "    pred_code = pipeline.predict([cleaned])[0]\n",
    "    pred_label = label_encoder.inverse_transform([pred_code])[0]\n",
    "    return {\n",
    "        \"input_text\": text,\n",
    "        \"cleaned_text\": cleaned,\n",
    "        \"sentiment\": pred_label,\n",
    "        \"prediction_code\": int(pred_code)\n",
    "    }\n",
    "\n",
    "np.random.seed(42)\n",
    "idxs = np.random.choice(len(X_test), size=10, replace=False)\n",
    "\n",
    "rows = []\n",
    "for i in idxs:\n",
    "    text_clean = X_test[i]\n",
    "    actual = le.inverse_transform([y_test[i]])[0]\n",
    "    pred = predict_sentiment(text_clean, best_pipe, le, do_preprocess=False)[\"sentiment\"]\n",
    "    rows.append({\"review_clean\": text_clean, \"actual\": actual, \"predicted\": pred, \"correct\": actual == pred})\n",
    "\n",
    "df_infer = pd.DataFrame(rows)\n",
    "df_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95b724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
